{
    "inference_server": {
        "base_url": "http://litellm:8000/v1",
        "api_key": "None"
    },
    "mcp_servers": {
        "gpt-researcher": {
            "command": "curl",
            "args": ["-s", "http://localhost:8000/mcp"]
        }
    },
    "network": {
        "host": "0.0.0.0",
        "port": 8080
    },
    "logging": {
        "log_level": "INFO"
    }
}
